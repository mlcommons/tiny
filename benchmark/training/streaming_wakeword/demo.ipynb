{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386153cd-684f-4bca-a6ad-eca4bcccfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op as frontend_op\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, pickle, sys, time\n",
    "import shutil\n",
    "import json\n",
    "from scipy.io import  wavfile\n",
    "from IPython import display\n",
    "\n",
    "import str_ww_util as util\n",
    "import get_dataset\n",
    "import keras_model as models\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6132467-f897-42fe-a324-edd03ba13f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db02f34-2ff4-43b2-b86d-0b46f3a4d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_dataset import get_data, get_file_lists, get_data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df306e-432a-4f12-a73a-cc945341dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter will pass an extra -f=<tmp_file> arg, which throws an \n",
    "# unrecognized argument error\n",
    "sys.argv = sys.argv[0:1] \n",
    "\n",
    "Flags = util.parse_command(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b889317-b130-4b06-97a6-93713873e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_mode = \"inference\" # \"inference\" OR \"short_training\" OR \"full_training\"\n",
    "\n",
    "if notebook_mode == \"inference\": \n",
    "  load_pretrained_model = True\n",
    "  Flags.num_samples_training = 2000 # we don't need the full set for inference\n",
    "  save_model = False\n",
    "elif notebook_mode == \"short_training\":\n",
    "  ## Set these for an extra short test just to validate that the code runs\n",
    "  load_pretrained_model = False\n",
    "  save_model = False\n",
    "  Flags.num_samples_training = 2000\n",
    "  Flags.num_samples_validation = 1000\n",
    "  Flags.num_samples_test = 1000\n",
    "  Flags.epochs = 10\n",
    "elif notebook_mode == \"full_training\":\n",
    "  ## Set these to for a full training run.\n",
    "  load_pretrained_model = False # True to load from a file, False to build/train from scratch\n",
    "  save_model = True\n",
    "else:\n",
    "  # Or make custom settings here\n",
    "  pass\n",
    "\n",
    "# 'trained_models/str_ww_model.h5' is the default save path for train.py\n",
    "pretrained_model_path = 'trained_models/str_ww_ref_model.h5' # path to load from if load_pretrained_model is True\n",
    "\n",
    "samp_freq = Flags.sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d86f7b-f4de-4b5e-9aed-85ddd5862714",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('streaming_config.json', 'r') as fpi:\n",
    "        streaming_config = json.load(fpi)\n",
    "    Flags.data_dir = streaming_config['speech_commands_path']\n",
    "except:\n",
    "    raise RuntimeError(\"\"\"\n",
    "        In this directory, copy streaming_config_template.json to streaming_config.json\n",
    "        and edit it to point to the directories where you have the speech commands dataset\n",
    "        and (optionally) the MUSAN noise data set.\n",
    "        \"\"\")\n",
    "Flags.bg_path = Flags.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1aef4-9059-440a-8174-b685ddde8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test, ds_val = get_dataset.get_all_datasets(Flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb33f3-9951-48b3-a54b-e83855c52846",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i,(x,y) in enumerate(ds_train.unbatch()):\n",
    "  if i < 5:\n",
    "    plt.subplot(5,1,i+1)\n",
    "    plt.imshow(np.squeeze(x))\n",
    "  else:\n",
    "    max_val = np.max(x)\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f7176-6ce8-4a1b-b39f-96f5377b1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dat in ds_train.unbatch().take(1):\n",
    "  print(\"One element from the training set has shape:\")\n",
    "  print(f\"Input tensor shape: {dat[0].shape}\")\n",
    "  print(f\"Label shape: {dat[1].shape}\")\n",
    "  print(f\"Label : {dat[1]}\")\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec83efd-fde8-4511-9bf6-74071d3dc201",
   "metadata": {},
   "source": [
    "These next two cells can be quite slow in the current implementation, so uncomment if you want to see them.\n",
    "They\n",
    "1. Show spectra of some target words\n",
    "2. Count the distribution of classes in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791000c5-750d-4235-a27c-448b98c82c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_examples = 3\n",
    "target_count = 0\n",
    "\n",
    "plt.Figure(figsize=(10,4))\n",
    "for dat in ds_train.unbatch():\n",
    "  if np.argmax(dat[1]) == 0:\n",
    "    target_count += 1\n",
    "    ax = plt.subplot(max_target_examples, 1, target_count)\n",
    "    # display.display(display.Audio(dat[0].numpy(), rate=16000))\n",
    "\n",
    "    log_spec = dat[0].numpy().squeeze()\n",
    "    height = log_spec.shape[0]\n",
    "    width = log_spec.shape[1]\n",
    "    X = np.linspace(0, 1.0, num=width, dtype=float)\n",
    "    Y = range(height)\n",
    "    ax.pcolormesh(X, Y, np.squeeze(log_spec))\n",
    "    if target_count >= max_target_examples:\n",
    "      break\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafe720-6a04-4ac0-bf45-8bd8456b5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at the label breakdown in the training set\n",
    "print(get_dataset.count_labels(ds_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9bd7e-5a64-40d0-b237-243035fc5fe8",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d88172-df6d-422b-a9ec-a47da6958ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_pretrained_model:\n",
    "  print(f\"Loading pretrained model from {pretrained_model_path}\")\n",
    "  with tfmot.quantization.keras.quantize_scope(): # needed for the QAT wrappers\n",
    "    model = keras.models.load_model(pretrained_model_path)\n",
    "else:\n",
    "  print(f\"Building model from scratch\")\n",
    "  model = models.get_model(args=Flags, use_qat=Flags.use_qat) # compile step is done inside get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6d1855-1cc3-4487-a694-525f3349389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae82dd-3138-45b0-82d9-51a53b617b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_pretrained_model:\n",
    "  callbacks = util.get_callbacks(args=Flags)\n",
    "  train_hist = model.fit(ds_train, validation_data=ds_val, callbacks=callbacks,\n",
    "                         epochs=Flags.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2f3c8-32cc-4f9e-a876-b1345f1849f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_model:\n",
    "  model.save('trained_models/str_ww_model_nb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38404fa1-9f9f-4c56-a22e-bfa99cdbcc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_pretrained_model:\n",
    "  plt.subplot(2,1,1)\n",
    "  plt.semilogy(train_hist.epoch, train_hist.history['loss'], train_hist.history['val_loss'])\n",
    "  plt.legend(['training', 'validation'])\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.subplot(2,1,2)\n",
    "  plt.plot(train_hist.epoch, train_hist.history['categorical_accuracy'], train_hist.history['val_categorical_accuracy'])\n",
    "  plt.legend(['training', 'validation'])\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af70d4-dd90-4cc1-96b7-86d79f0f1d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell can be slow with QAT enabled\n",
    "print(f\"Eval on training set\")\n",
    "model.evaluate(ds_train)\n",
    "print(f\"Eval on validation set\")\n",
    "model.evaluate(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd9778-5bd9-4e25-ae50-18a93efe06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_plot_confusion_matrix(model, dataset, label_list=['marvin', 'silent', 'other']):\n",
    "  dataset = dataset.cache() # prevent shuffling between where we get predictions and where we get labels\n",
    "  model_out = model.predict(dataset)\n",
    "  model_out = np.squeeze(model_out)\n",
    "  y_pred_val = np.argmax(model_out, axis=1)\n",
    "  \n",
    "  y_true_val = np.nan*np.zeros(y_pred_val.shape[0])\n",
    "  for i,dat in enumerate(dataset.unbatch()):\n",
    "    y_true_val[i] = np.argmax(dat[1])\n",
    "  \n",
    "  acc = sum(y_pred_val == y_true_val) / len(y_true_val)\n",
    "  print(f'Accuracy: {acc:.1%}')\n",
    "\n",
    "  confusion_mtx = tf.math.confusion_matrix(y_true_val, y_pred_val) \n",
    "  plt.figure(figsize=(6, 6))\n",
    "  sns.heatmap(confusion_mtx, xticklabels=label_list, yticklabels=label_list, \n",
    "              annot=True, fmt='g')\n",
    "  plt.gca().invert_yaxis() # flip so origin is at bottom left\n",
    "  plt.xlabel('Prediction')\n",
    "  plt.ylabel('Label')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2407081-1022-4147-b6d3-d489a33f0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_and_plot_confusion_matrix(model, ds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd72136a-5314-4cb4-a96d-41f44525cd29",
   "metadata": {},
   "source": [
    "## Post-Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45495640-7e98-474e-910f-6070f157ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python quantize.py --saved_model_path=trained_models/str_ww_ref_model.h5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d8765-135c-45c2-8254-6de51deeec8c",
   "metadata": {},
   "source": [
    "#### Test Quantized Interpreter on One Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9667f-e5f0-4d20-a18b-91203ac8e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tfl_file_name)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']\n",
    "output_data = []\n",
    "labels = []\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "output_scale, output_zero_point = output_details[0][\"quantization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49f1eb-539c-4df4-80ed-cfe497a501f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec, label = next(ds_val.unbatch().batch(1).take(1).as_numpy_iterator())\n",
    "\n",
    "spec_q = np.array(spec/input_scale + input_zero_point, dtype=np.int8)\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], spec_q)\n",
    "interpreter.invoke()\n",
    "out_vec = interpreter.get_tensor(output_details[0]['index'])\n",
    "pred_label = np.argmax(out_vec)\n",
    "out_vec_dequant = (out_vec.astype(np.float32) - output_zero_point)*output_scale\n",
    "\n",
    "print(f\"Output = {out_vec}.\")\n",
    "print(f\"True (vs predicted) label = {np.argmax(label)} (vs {pred_label})\")\n",
    "print(f\"Dequantized output vector = {out_vec_dequant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63f5ed-87d6-4591-a066-a8b6849a86fc",
   "metadata": {},
   "source": [
    "#### Now Measure Quantized Accuracy on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b2884-42dd-4d35-a0b7-adcd9baf4a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time(); \n",
    "predictions = []\n",
    "labels = []\n",
    "# num_test_samples = 100\n",
    "\n",
    "eval_ds = ds_val # ds_val OR ds_train OR ds_test\n",
    "\n",
    "for next_spec, next_label in eval_ds.unbatch().batch(1): # .take(num_test_samples):    \n",
    "  spec_q = np.array(next_spec/input_scale + input_zero_point, dtype=np.int8)\n",
    "  interpreter.set_tensor(input_details[0]['index'], spec_q)\n",
    "  interpreter.invoke()\n",
    "  # The function `get_tensor()` returns a copy of the tensor data.\n",
    "  # Use `tensor()` in order to get a pointer to the tensor.\n",
    "  predictions.append(np.argmax(interpreter.get_tensor(output_details[0]['index'])))\n",
    "  labels.append(next_label[0])\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "labels = np.argmax(np.array(labels), axis=1)\n",
    "num_correct = np.sum(labels == predictions)\n",
    "acc = num_correct / len(labels)\n",
    "print(f\"Accuracy = {acc:5.3f} ({num_correct}/{len(labels)})\")\n",
    "t1 = time.time(); \n",
    "print(f\"Measured validation accuracy in {t1-t0} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7100ecee-9ed6-42b4-984f-8b75ad642d11",
   "metadata": {},
   "source": [
    "As of 12 Aug 2024, the quantized accuracy on the validation set is 95.5%.  Now we can plot the confusion matrix of the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25950850-87f2-4dda-9965-776f68c25f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['marvin', 'silent', 'other']\n",
    "confusion_mtx = tf.math.confusion_matrix(labels, predictions)\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(confusion_mtx, xticklabels=label_list, yticklabels=label_list, \n",
    "            annot=True, fmt='g')\n",
    "plt.gca().invert_yaxis() # flip so origin is at bottom left\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2750b5-4aa6-4613-bbc9-ddca78fe6cdd",
   "metadata": {},
   "source": [
    "## Run Model on Long Waveform\n",
    "\n",
    "The use case this benchmark is meant to model is one of detecting a \"wakeword\" (similar to \"Hey Siri\", \"Alexa\", or \"OK Google\") in a continuous stream of sound, including background noise.  So to mimic that use case, we will run the model on a longer waveform that includes several instances of the wakeword (\"Marvin\") and some background noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cfe55f-389f-4256-b6f0-c5cc83d2a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if streaming_config['musan_path'] is None or len(streaming_config['musan_path']) == 0:\n",
    "  raise RuntimeError(\"Stopping before the long-wave test, which requires the musan dataset in streaming_config['musan_path']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dfb8a9-f544-482c-a90b-11d9e1fee189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_and_normalize(wav_in, rel_thresh):\n",
    "  \"\"\"\n",
    "  Trims leading and trailing 'quiet' segments, where quiet is defined as \n",
    "  less than rel_thresh*max(wav_in).\n",
    "  Then scales such that RMS of trimmed wav = 1.0\n",
    "  \"\"\"\n",
    "  idx_start = np.min(np.nonzero(ww_wav > np.max(ww_wav)*rel_thresh))\n",
    "  idx_stop  = np.max(np.nonzero(ww_wav > np.max(ww_wav)*rel_thresh))\n",
    "  \n",
    "  wav_out = wav_in[idx_start:idx_stop]\n",
    "  wav_out = wav_out / np.std(wav_out) \n",
    "  return wav_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e9c14e-48b6-4c1c-97c4-ee24016f64d9",
   "metadata": {},
   "source": [
    "For the keras model, we can build an alternate version of the model that accepts inputs of arbitrary length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102a02f-7b4b-4643-8c54-795784b9def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_uses_qat = hasattr(model.layers[1], \"quantizer\")\n",
    "Flags.variable_length=True\n",
    "model_tv = models.get_model(args=Flags, use_qat=pretrained_model_uses_qat)\n",
    "Flags.variable_length=False\n",
    "# transfer weights from trained model into variable-length model\n",
    "model_tv.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d4877-e789-49ab-ac73-92065a747446",
   "metadata": {},
   "source": [
    "## Run Streaming Test on Long Waveform\n",
    "\n",
    "A pre-constructed test wav is included in the repo (`long_wav.wav`) along with a json file that indicates the beginning and end of every instance of the wakeword, `long_wav_ww_windows.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f58ec7-8d4c-4ecf-aeb5-fa782ca47941",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_sampling_freq, long_wav = wavfile.read(\"long_wav.wav\")\n",
    "assert wav_sampling_freq == samp_freq\n",
    "t = np.arange(len(long_wav))/samp_freq\n",
    "print(f\"min/max of long wav = {np.min(long_wav)} / {np.max(long_wav)}\")\n",
    "long_wav = long_wav / 2**15 # scale into [-1.0, +1.0] range\n",
    "\n",
    "ww_windows_file = 'long_wav_ww_windows.json' # start/stop points of wakewords in a list\n",
    "with open(ww_windows_file, 'r') as fpi:\n",
    "  ww_windows = json.load(fpi)\n",
    "  \n",
    "# construct waveform that shows when wakeword is present\n",
    "ww_present = np.zeros(len(long_wav))\n",
    "for t_start, t_stop in ww_windows:\n",
    "  idx_start = int(t_start*samp_freq)\n",
    "  idx_stop  = int(t_stop*samp_freq)\n",
    "  ww_present[idx_start:idx_stop] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee69ba-3130-4235-b1d2-ab751a25b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build a feature extractor that can operate on longer waveforms.\n",
    "## this one can operate on waveforms up to len(long_wav)\n",
    "data_config_long = get_dataset.get_data_config(Flags, 'training')\n",
    "\n",
    "with open(\"data_config_nb.json\", \"w\") as fpo:\n",
    "    json.dump(data_config_long, fpo, indent=4)\n",
    "              \n",
    "# long_wav = long_wav / np.max(np.abs(long_wav)) # scale into [-1.0, +1.0] range\n",
    "t = np.arange(len(long_wav))/samp_freq\n",
    "\n",
    "feature_extractor_long = get_dataset.get_lfbe_func(data_config_long)\n",
    "# the feature extractor needs a label (in 1-hot format), but it doesn't matter what it is\n",
    "# long_spec = feature_extractor_long({'audio':long_wav, 'label':[0.0, 0.0, 0.0]})['audio'].numpy()\n",
    "\n",
    "# long_spec = feature_extractor_long(np.expand_dims(long_wav, 0)).numpy()\n",
    "long_spec = feature_extractor_long(long_wav).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81014678-1bbe-4b59-adf0-70dea14dcd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_spec_from_file = np.load(\"long_specgram.npz\")[\"specgram\"]\n",
    "specgrams_match = np.allclose(long_spec, long_spec_from_file, atol=1e-6)\n",
    "\n",
    "print(f\"Does spectrogram loaded from file match the one we created?: {specgrams_match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97becd93-f5c3-4998-af50-f43bd6fd5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll count a detection when the softmax output for the wakeword exceeds the detection threshold det_thresh\n",
    "det_thresh = 0.95\n",
    "\n",
    "yy = model_tv(np.expand_dims(long_spec, 0))[0].numpy()\n",
    "## shows detection when ww activation > thresh\n",
    "ww_detected_spec_scale = (yy[:,0]>det_thresh).astype(int)\n",
    "ww_true_detects, ww_false_detects, ww_false_rejects = util.get_true_and_false_detections(ww_detected_spec_scale, ww_present, Flags)\n",
    "\n",
    "print(f\"{np.sum(ww_false_detects!=0)} false detections.\")\n",
    "print(f\"{np.sum(ww_true_detects!=0)} true detections.\")\n",
    "print(f\"{np.sum(ww_false_rejects!=0)} false rejections.\")\n",
    "\n",
    "plt.plot(t, 1*util.zero2nan(ww_false_detects),   'rx', label='False Detections')\n",
    "plt.plot(t, 2*(util.zero2nan(ww_true_detects)),  'g*', label='True Detections')\n",
    "plt.plot(t, 3*(util.zero2nan(ww_false_rejects)), 'bo', label='False Rejections')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb6fe0-1c4e-4321-bb62-0b5ff05d2efa",
   "metadata": {},
   "source": [
    "Take a look at some of the false positives here, and then in the next cell, some of the false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c516d7-fd04-43e6-bfb2-0c92736f7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fp_clips_to_show = np.minimum(5, np.sum(ww_false_detects, dtype=np.int32))\n",
    "for i in range(num_fp_clips_to_show):\n",
    "  fp_start = np.nonzero(ww_false_detects)[0][i] # sample number where the false pos starts\n",
    "  print(f\"False positive at {fp_start/samp_freq:3.2f}s (sample {fp_start})\")\n",
    "  fp_clip = slice(fp_start-16000,fp_start+16000) # add 2s before and after\n",
    "  display.display(display.Audio(long_wav[fp_clip], rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ea3db-f803-4427-9efa-10c565f4eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fn_clips_to_show = np.minimum(5, np.sum(ww_false_rejects, dtype=np.int32))\n",
    "for i in range(num_fn_clips_to_show):\n",
    "  fn_start = np.nonzero(ww_false_rejects)[0][i]\n",
    "  print(f\"False negative at {fn_start/samp_freq:3.2f}s (sample {fn_start})\")\n",
    "  fn_clip = slice(fn_start-16000,fn_start+16000)\n",
    "  display.display(display.Audio(long_wav[fn_clip], rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c2abe-1f3a-4250-b25e-ae5c059fb7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_clip(wav_clip, model, feature_extractor):\n",
    "    # clip_spec = feature_extractor({'audio':wav_clip, 'label':[0.0, 0.0, 0.0]})['audio'].numpy()\n",
    "    clip_spec = feature_extractor(wav_clip).numpy()\n",
    "    spec_len = int(1.1*len(wav_clip)/(Flags.sample_rate*Flags.window_stride_ms/1000))\n",
    "    clip_spec = clip_spec[:spec_len,:]\n",
    "    yy_clip = model(np.expand_dims(clip_spec, 0))[0].numpy()\n",
    "\n",
    "    plt.subplot(3,1,1)\n",
    "    t_wav= np.arange(len(wav_clip))/Flags.sample_rate\n",
    "    plt.plot(t_wav, wav_clip)\n",
    "                     \n",
    "    plt.subplot(3,1,2)\n",
    "    # plt.imshow(clip_spec.squeeze().T, origin=\"lower\", aspect=\"auto\");\n",
    "    t_spec= np.arange(clip_spec.shape[0])*(Flags.window_stride_ms/1000)\n",
    "    mels = np.arange(clip_spec.shape[-1])\n",
    "    plt.pcolormesh(t_spec, mels, clip_spec.squeeze().T)\n",
    "  \n",
    "    plt.subplot(3,1,3)\n",
    "    t_yy= np.arange(yy_clip.shape[0])*(Flags.window_stride_ms/1000)\n",
    "    print(f\"t_yy shape = {t_yy.shape}, yy_clip shape = {yy_clip.shape}\")\n",
    "    \n",
    "    plt.plot(t_yy, yy_clip, [0,t_yy[-1]], [det_thresh, det_thresh]);\n",
    "    plt.legend(label_list+[f\"Threshold ({det_thresh})\"], loc='lower left', fontsize=8);\n",
    "   \n",
    "    display.display(display.Audio(wav_clip, rate=16000))\n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c9adcc-7a75-4297-8e29-755f494cd0a4",
   "metadata": {},
   "source": [
    "Now we can take a closer look at one of the errors, showing the waveform plot, listening to the audio, and showing the spectrogram, along with the model outputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfc9ab-0de7-482e-8a8a-7d79ed8a48b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_idx = 0  # which false pos/neg to look at\n",
    "# you can replace ww_false_detects with ww_false_rejects to view false negatives\n",
    "clip_idx = np.nonzero(ww_false_rejects)[0][error_idx] \n",
    "clip_range = slice(clip_idx-2*Flags.sample_rate,clip_idx+2*Flags.sample_rate)\n",
    "print(f\"Examining clip from {clip_range.start} to {clip_range.stop}\")\n",
    "wav_clip = long_wav[clip_range]\n",
    "examine_clip(wav_clip, model_tv, feature_extractor_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19666bc5-4b43-41ef-b93d-050ff7f87dd0",
   "metadata": {},
   "source": [
    "## Quantized Model on Long Wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde0e4f-c30e-45c1-aa4f-8afe5b3ef76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_q = np.nan*np.zeros((long_spec.shape[0]-model.input.shape[1]+1,3))\n",
    "t0 = time.time(); \n",
    "preds_q = []\n",
    "labels = []\n",
    "\n",
    "for idx in range(long_spec.shape[0]-model.input.shape[1]+1):\n",
    "  spec = long_spec[idx:idx+input_shape[1],:,:]\n",
    "  spec = np.expand_dims(spec, 0) # add batch dimension  \n",
    "  spec_q = np.array(spec/input_scale + input_zero_point, dtype=np.int8)\n",
    "  \n",
    "  interpreter.set_tensor(input_details[0]['index'], spec_q)\n",
    "  interpreter.invoke()\n",
    "  # The function `get_tensor()` returns a copy of the tensor data.\n",
    "  # Use `tensor()` in order to get a pointer to the tensor.\n",
    "  yy_q[idx,:] = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Dequantize so that softmax output is in range [0,1]\n",
    "yy_q = (yy_q.astype(np.float32) - output_zero_point)*output_scale\n",
    "\n",
    "t1 = time.time(); \n",
    "print(f\"Ran quantized model on long wave in {t1-t0:1.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd6f83-22d5-463f-ae1c-1e49a2d2486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_thresh = 0.95\n",
    "## shows detection when wakeword activation is strongest output\n",
    "# ww_detected_spec_scale = (np.argmax(yy, axis=1)==0) # detections on the time scale of spectrograms\n",
    "\n",
    "## shows detection when ww activation > thresh\n",
    "ww_detected_spec_scale = (yy_q[:,0]>det_thresh).astype(int)\n",
    "\n",
    "ww_true_detects, ww_false_detects, ww_false_rejects = util.get_true_and_false_detections(ww_detected_spec_scale, ww_present, Flags)\n",
    "\n",
    "print(f\"{np.sum(ww_false_detects!=0)} false detections.\")\n",
    "print(f\"{np.sum(ww_true_detects!=0)} true detections.\")\n",
    "print(f\"{np.sum(ww_false_rejects!=0)} false rejections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89671bb6-294b-4ead-949d-d38891b2a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, 1*util.zero2nan(ww_false_detects),   'rx', label='False Detections')\n",
    "plt.plot(t, 2*(util.zero2nan(ww_true_detects)),  'g*', label='True Detections')\n",
    "plt.plot(t, 3*(util.zero2nan(ww_false_rejects)), 'bs', label='False Rejections')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419ef55-1132-425c-91b9-534bfe7679de",
   "metadata": {},
   "source": [
    "## Scratch Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09ab20-9aef-44ab-91d8-b63713ec31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_layer_weights(model):\n",
    "    for layer in model.layers:\n",
    "        # Get the layer's weights as a list of numpy arrays\n",
    "        weights = layer.get_weights()\n",
    "        print(f\"{layer.name:20} ({layer.__class__.__name__:18})\", end=\"\")\n",
    "        if weights:  # Check if the layer has weights\n",
    "            # Flatten and concatenate all weight tensors for the layer\n",
    "            all_weights = np.concatenate([w.ravel() for w in weights])\n",
    "            \n",
    "            # Get the largest absolute value in the weights\n",
    "            max_abs_value = np.max(np.abs(all_weights))\n",
    "            \n",
    "            if max_abs_value > 0:  # Avoid division by zero\n",
    "                # Compute thresholds\n",
    "                thresholds = [0.001 * max_abs_value, 0.01 * max_abs_value, 0.02 * max_abs_value]\n",
    "                \n",
    "                # Compute the percentages of weights below each threshold\n",
    "                percentages = [(np.abs(all_weights) < t).mean() * 100 for t in thresholds]\n",
    "                \n",
    "                # Print results for the layer\n",
    "                print(f\"Sparsity at 0.1% = {percentages[0]:.2f}%\",end=\"\")\n",
    "                print(f\" 1% = {percentages[1]:.2f}%\",end=\"\")\n",
    "                print(f\" 2% = {percentages[2]:.2f}%\")\n",
    "            else:\n",
    "                print(f\" - All weights are zero\")\n",
    "        else:\n",
    "            print(f\" - No weights\")\n",
    "    all_model_weights = np.concatenate([w.ravel() for w in model.get_weights()])\n",
    "    max_abs_value = np.max(np.abs(all_model_weights))\n",
    "    if max_abs_value > 0:  # Avoid division by zero\n",
    "        # Compute thresholds\n",
    "        thresholds = [0.001 * max_abs_value, 0.01 * max_abs_value, 0.02 * max_abs_value]\n",
    "        \n",
    "        # Compute the percentages of weights below each threshold\n",
    "        percentages = [(np.abs(all_model_weights) < t).mean() * 100 for t in thresholds]\n",
    "        \n",
    "        # Print results for the layer\n",
    "        print(f\"{'Full model':20} {20*' '}\", end=\"\")\n",
    "        print(f\"Sparsity at 0.1% = {percentages[0]:.2f}%\",end=\"\")\n",
    "        print(f\" 1% = {percentages[1]:.2f}%\",end=\"\")\n",
    "        print(f\" 2% = {percentages[2]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb60caf-f1d2-4b4e-a13e-78c7eff3f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_layer_weights(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
